{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from fastparquet import ParquetFile,write\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from tqdm import tqdm\n",
    "\n",
    "from cm2df import cm2df,precision_recall_fscore_support_metrics2df\n",
    "from sklearn.metrics import confusion_matrix, classification_report,precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf=ParquetFile('subset_feature_4ML_110619.parq')\n",
    "udf=pf.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 32880 entries, 11049447 to 9968570\n",
      "Data columns (total 35 columns):\n",
      "category                 32880 non-null int64\n",
      "past                     32880 non-null int32\n",
      "votes                    32880 non-null int64\n",
      "is_eventbrite            32880 non-null int32\n",
      "is_free                  32880 non-null int32\n",
      "doors                    32880 non-null int32\n",
      "sold_out                 32880 non-null int32\n",
      "venue.id                 32880 non-null int64\n",
      "venue.popularity         32880 non-null float64\n",
      "venue.zip                32880 non-null int64\n",
      "ticket_allages           32880 non-null int32\n",
      "ticket_price_low         32880 non-null float64\n",
      "ticket_price_max         32880 non-null float64\n",
      "min_age                  32880 non-null int32\n",
      "artist.popularity.sum    32880 non-null float64\n",
      "artist.popularity.avg    32880 non-null float64\n",
      "artist.popularity.max    32880 non-null float64\n",
      "dow                      32880 non-null int64\n",
      "doy                      32880 non-null int64\n",
      "month                    32880 non-null int64\n",
      "day                      32880 non-null int64\n",
      "hour                     32880 non-null int64\n",
      "venue.tol_num_events     32880 non-null int64\n",
      "ev_id                    32880 non-null int64\n",
      "title                    32880 non-null object\n",
      "venue.title              32880 non-null object\n",
      "venue.address            32806 non-null object\n",
      "venue.city               32880 non-null object\n",
      "venue.state              32880 non-null object\n",
      "venue.latitude           32880 non-null float64\n",
      "venue.longitude          32880 non-null float64\n",
      "duration                 32880 non-null float64\n",
      "duration_day             32880 non-null float64\n",
      "multiday                 32880 non-null bool\n",
      "avg_votes_pday           32880 non-null float64\n",
      "dtypes: bool(1), float64(11), int32(7), int64(11), object(5)\n",
      "memory usage: 7.9+ MB\n"
     ]
    }
   ],
   "source": [
    "udf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf['multiday'] = udf['multiday'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "usecolsX=['category', 'past', 'is_eventbrite', 'is_free', 'doors',\n",
    "       'sold_out', 'venue.id', 'venue.popularity', 'venue.zip',\n",
    "       'ticket_allages', 'ticket_price_low', 'ticket_price_max', 'min_age',\n",
    "       'artist.popularity.sum', 'artist.popularity.avg',\n",
    "       'artist.popularity.max', 'dow', 'doy', 'month', 'day', 'hour',\n",
    "       'venue.tol_num_events', 'duration', 'duration_day', 'multiday']\n",
    "usecoly1=['votes']\n",
    "usecoly2=['avg_votes_pday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to create udf_y based on 3 classes: low, mid, high\n",
    "def get_class(df,thre1,thre2):\n",
    "    n=len(df)\n",
    "    newdf=[]\n",
    "    cut1=min([thre1,thre2])\n",
    "    cut2=max([thre1,thre2])\n",
    "    for item in df:\n",
    "        if item<cut1: newdf+=[0]\n",
    "        elif (item<cut2) and (item>=cut1):newdf+=[1]\n",
    "        else: newdf+=[2]\n",
    "    return newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import algorithms for model comparisons\n",
    "# tree models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "# scaling of the features\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Model: Random Forest> + SMOTE Upsampling high votes to same as medium votes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_X=udf[usecolsX+['votes']]\n",
    "udf_y=udf[usecoly1]\n",
    "y=get_class(udf_y.values,80,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in the training data:\n",
      "number of low votes: 22128\n",
      "number of medium votes: 741\n",
      "number of high votes: 147\n",
      "in the testing data:\n",
      "number of low votes: 9483\n",
      "number of medium votes: 318\n",
      "number of high votes: 63\n"
     ]
    }
   ],
   "source": [
    "# first spare the training data for upsampling\n",
    "X_train, X_test, y_train, y_test = train_test_split(udf_X, y, test_size=0.3, stratify=y, random_state=12)\n",
    "print(\"in the training data:\")\n",
    "print(\"number of low votes:\",list(y_train).count(0))\n",
    "print(\"number of medium votes:\",list(y_train).count(1))\n",
    "print(\"number of high votes:\",list(y_train).count(2))\n",
    "print(\"in the testing data:\")\n",
    "print(\"number of low votes:\",list(y_test).count(0))\n",
    "print(\"number of medium votes:\",list(y_test).count(1))\n",
    "print(\"number of high votes:\",list(y_test).count(2))\n",
    "# do upsample with replacement\n",
    "from sklearn.utils import resample\n",
    "# separate three classes\n",
    "# add back the 'votes' column, and redo the sampling\n",
    "mask0=[True if x==0 else False for x in y_train]\n",
    "mask1=[True if x==1 else False for x in y_train]\n",
    "mask2=[True if x==2 else False for x in y_train]\n",
    "class0=X_train.loc[mask0]\n",
    "class1=X_train.loc[mask1]\n",
    "class2=X_train.loc[mask2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_rec=[]\n",
    "prec_rec=[]\n",
    "recal_rec=[]\n",
    "max_f=len(udf_X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE upsample of the high votes: upsample high votes to be same as medium (3.5 times more than original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE \n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the class1 and class2 to dataframe and then input to SMOTE:\n",
    "tmpdf=pd.concat([class1,class2])\n",
    "# re-shuffle the order\n",
    "tmpdf=tmpdf.sample(frac=1)\n",
    "\n",
    "y_new=get_class(tmpdf['votes'].values,80,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resampled, y_resampled = SMOTE().fit_resample(tmpdf, y_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 741), (2, 741)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(Counter(y_resampled).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_12_tol = pd.DataFrame(X_resampled, columns=usecolsX+['votes'])\n",
    "# add back the dataframes and reshuffle\n",
    "ndf=pd.concat([df_12_tol,class0])\n",
    "ndf=ndf.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new y-class\n",
    "yy=get_class(ndf['votes'].values,80,300)\n",
    "ndf=ndf[usecolsX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "increase sample size on the percentage: 2.5808133472367047 %\n"
     ]
    }
   ],
   "source": [
    "print(\"increase sample size on the percentage:\",(len(ndf)-len(X_train))/len(X_train)*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in the upsampled training data:\n",
      "number of low votes: 22128\n",
      "number of medium votes: 741\n",
      "number of high votes: 741 ,compared with original: 210\n",
      "number of features in X: 25\n"
     ]
    }
   ],
   "source": [
    "print(\"in the upsampled training data:\")\n",
    "print(\"number of low votes:\",list(yy).count(0))\n",
    "print(\"number of medium votes:\",list(yy).count(1))\n",
    "print(\"number of high votes:\",list(yy).count(2),\",compared with original:\",list(y).count(2))\n",
    "print(\"number of features in X:\",len(ndf.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redo the benchmark model\n",
    "scalar = MinMaxScaler()\n",
    "scalar.fit(ndf)\n",
    "X = scalar.transform(ndf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spare the X_test with proper versions\n",
    "X_test=X_test[usecolsX]\n",
    "X_test=scalar.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec_rec=[]\n",
    "recal_rec=[]\n",
    "wf1_rec=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted f1 score: 0.4916849365684099\n"
     ]
    }
   ],
   "source": [
    "classifier0=RandomForestClassifier(n_estimators=90,random_state=12,max_features=int(max_f*0.75),verbose=0)\n",
    "classifier0.fit(X,yy)\n",
    "pred_y_test=classifier0.predict(X_test)\n",
    "# assess the model performance: obtain prescision/recall/f1 scores\n",
    "a=precision_recall_fscore_support(y_test, pred_y_test)\n",
    "f1s=a[2]\n",
    "prec_rec+=[a[0]]\n",
    "recal_rec+=[a[1]]\n",
    "# create the weighted f1 score: larger emphasis on high-votes bin and medium\n",
    "# adjusted f1_forall= (f1_low+f1_mid*2+f1_high*3)/6\n",
    "adjf1=(f1s[0]+f1s[1]*2+f1s[2]*3)/6\n",
    "print(\"weighted f1 score:\",adjf1)\n",
    "wf1_rec+=[adjf1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[9438   32   13]\n",
      " [ 206  105    7]\n",
      " [  31   14   18]]\n",
      "other metrics:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      9483\n",
      "           1       0.70      0.33      0.45       318\n",
      "           2       0.47      0.29      0.36        63\n",
      "\n",
      "    accuracy                           0.97      9864\n",
      "   macro avg       0.71      0.54      0.60      9864\n",
      "weighted avg       0.96      0.97      0.96      9864\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print confusion matrix-- testing\n",
    "print(\"confusion matrix:\",confusion_matrix(y_test, pred_y_test))\n",
    "print(\"other metrics:\",classification_report(y_test, pred_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'bootstrap': True, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 19, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 90, 'n_jobs': None, 'oob_score': False, 'random_state': 12, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "# Look at parameters used by our current forest\n",
    "print('Parameters currently in use:\\n')\n",
    "print(classifier0.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Hyper-parameter tunning of the RF model:\n",
    "##### * target parameters: (1) n_estimator: number of trees\n",
    "#####                      (2) criterion: gini or entropy\n",
    "#####                      (3) max_depth: The maximum depth of the tree\n",
    "#####                      (4) min_samples_split: 2 or above , min number to split\n",
    "#####                      (5) min_samples_leaf : 1 or above, min number on per leaf\n",
    "#####                      (6) max_features: maximum feature number to randomly use per split, 25 or below\n",
    "#####                      (7) bootstrap : Whether bootstrap samples are used when building trees, True/False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [90, 120, 160, 200, 300], 'criterion': ['gini', 'entropy'], 'max_features': [10, 15, 20, 25], 'max_depth': [12, 15, 18, 20, 22, 25], 'min_samples_leaf': [1, 3, 5, 7, 9], 'min_samples_split': [2, 5, 8, 10, 20], 'bootstrap': [True, False]}\n"
     ]
    }
   ],
   "source": [
    "# Create dictionary hyperparameter candidates\n",
    "n_est=[90,120,160,200,300]\n",
    "crit=['gini','entropy']\n",
    "max_d=[5,15,25,40,100]\n",
    "min_sl=[1,5,9,15,35]\n",
    "min_ss=[2,8,15,35]\n",
    "max_f=[5,10,15,20,25]\n",
    "bots = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "hpparam = {'n_estimators': n_est,\n",
    "           'criterion': crit,\n",
    "           'max_features': max_f,\n",
    "           'max_depth': max_d,\n",
    "           'min_samples_leaf': min_sl,\n",
    "           'min_samples_split': min_ss,\n",
    "           'bootstrap': bots}\n",
    "print(hpparam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accuracy', 'adjusted_mutual_info_score', 'adjusted_rand_score', 'average_precision', 'balanced_accuracy', 'brier_score_loss', 'completeness_score', 'explained_variance', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'fowlkes_mallows_score', 'homogeneity_score', 'jaccard', 'jaccard_macro', 'jaccard_micro', 'jaccard_samples', 'jaccard_weighted', 'max_error', 'mutual_info_score', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_median_absolute_error', 'normalized_mutual_info_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc', 'v_measure_score']\n"
     ]
    }
   ],
   "source": [
    "# get valid scorers , check options:\n",
    "from sklearn import metrics\n",
    "print(sorted(metrics.SCORERS.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random = RandomizedSearchCV(estimator = RandomForestClassifier(), param_distributions = hpparam, scoring='f1_micro',n_iter = 120, cv = 3, verbose=2, random_state=42, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 120 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   49.0s\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:  6.1min\n",
      "[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed: 15.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "                   estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                    class_weight=None,\n",
       "                                                    criterion='gini',\n",
       "                                                    max_depth=None,\n",
       "                                                    max_features='auto',\n",
       "                                                    max_leaf_nodes=None,\n",
       "                                                    min_impurity_decrease=0.0,\n",
       "                                                    min_impurity_split=None,\n",
       "                                                    min_samples_leaf=1,\n",
       "                                                    min_samples_split=2,\n",
       "                                                    min_weight_fraction_leaf=0.0,\n",
       "                                                    n_estimators='warn',\n",
       "                                                    n_jobs=None,\n",
       "                                                    oob_sc...\n",
       "                   iid='warn', n_iter=120, n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'criterion': ['gini', 'entropy'],\n",
       "                                        'max_depth': [12, 15, 18, 20, 22, 25],\n",
       "                                        'max_features': [10, 15, 20, 25],\n",
       "                                        'min_samples_leaf': [1, 3, 5, 7, 9],\n",
       "                                        'min_samples_split': [2, 5, 8, 10, 20],\n",
       "                                        'n_estimators': [90, 120, 160, 200,\n",
       "                                                         300]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=False, scoring='f1_micro', verbose=2)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.fit(X,yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 120,\n",
       " 'min_samples_split': 5,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_features': 10,\n",
       " 'max_depth': 22,\n",
       " 'criterion': 'gini',\n",
       " 'bootstrap': False}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=rf_random.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "rslt_df=pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "      <th>param_min_samples_leaf</th>\n",
       "      <th>param_max_features</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_criterion</th>\n",
       "      <th>param_bootstrap</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25.409412</td>\n",
       "      <td>1.165219</td>\n",
       "      <td>0.200134</td>\n",
       "      <td>0.014575</td>\n",
       "      <td>90</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>gini</td>\n",
       "      <td>False</td>\n",
       "      <td>{'n_estimators': 90, 'min_samples_split': 20, ...</td>\n",
       "      <td>0.960864</td>\n",
       "      <td>0.965820</td>\n",
       "      <td>0.960102</td>\n",
       "      <td>0.962262</td>\n",
       "      <td>0.002535</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.685415</td>\n",
       "      <td>0.345324</td>\n",
       "      <td>0.208441</td>\n",
       "      <td>0.026994</td>\n",
       "      <td>90</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>15</td>\n",
       "      <td>gini</td>\n",
       "      <td>True</td>\n",
       "      <td>{'n_estimators': 90, 'min_samples_split': 8, '...</td>\n",
       "      <td>0.959848</td>\n",
       "      <td>0.963914</td>\n",
       "      <td>0.963278</td>\n",
       "      <td>0.962346</td>\n",
       "      <td>0.001786</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.693385</td>\n",
       "      <td>0.398346</td>\n",
       "      <td>0.208117</td>\n",
       "      <td>0.015020</td>\n",
       "      <td>90</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>22</td>\n",
       "      <td>entropy</td>\n",
       "      <td>True</td>\n",
       "      <td>{'n_estimators': 90, 'min_samples_split': 10, ...</td>\n",
       "      <td>0.965184</td>\n",
       "      <td>0.966836</td>\n",
       "      <td>0.965184</td>\n",
       "      <td>0.965735</td>\n",
       "      <td>0.000779</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.243274</td>\n",
       "      <td>0.203947</td>\n",
       "      <td>0.288895</td>\n",
       "      <td>0.023209</td>\n",
       "      <td>120</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>22</td>\n",
       "      <td>entropy</td>\n",
       "      <td>True</td>\n",
       "      <td>{'n_estimators': 120, 'min_samples_split': 10,...</td>\n",
       "      <td>0.963405</td>\n",
       "      <td>0.966201</td>\n",
       "      <td>0.965947</td>\n",
       "      <td>0.965184</td>\n",
       "      <td>0.001262</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72.730021</td>\n",
       "      <td>5.825824</td>\n",
       "      <td>0.638295</td>\n",
       "      <td>0.090327</td>\n",
       "      <td>300</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>entropy</td>\n",
       "      <td>False</td>\n",
       "      <td>{'n_estimators': 300, 'min_samples_split': 8, ...</td>\n",
       "      <td>0.953240</td>\n",
       "      <td>0.951715</td>\n",
       "      <td>0.952986</td>\n",
       "      <td>0.952647</td>\n",
       "      <td>0.000667</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      25.409412      1.165219         0.200134        0.014575   \n",
       "1      14.685415      0.345324         0.208441        0.026994   \n",
       "2      15.693385      0.398346         0.208117        0.015020   \n",
       "3      12.243274      0.203947         0.288895        0.023209   \n",
       "4      72.730021      5.825824         0.638295        0.090327   \n",
       "\n",
       "  param_n_estimators param_min_samples_split param_min_samples_leaf  \\\n",
       "0                 90                      20                      1   \n",
       "1                 90                       8                      9   \n",
       "2                 90                      10                      1   \n",
       "3                120                      10                      5   \n",
       "4                300                       8                      7   \n",
       "\n",
       "  param_max_features param_max_depth param_criterion param_bootstrap  \\\n",
       "0                 20              18            gini           False   \n",
       "1                 20              15            gini            True   \n",
       "2                 25              22         entropy            True   \n",
       "3                 15              22         entropy            True   \n",
       "4                 25              25         entropy           False   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'n_estimators': 90, 'min_samples_split': 20, ...           0.960864   \n",
       "1  {'n_estimators': 90, 'min_samples_split': 8, '...           0.959848   \n",
       "2  {'n_estimators': 90, 'min_samples_split': 10, ...           0.965184   \n",
       "3  {'n_estimators': 120, 'min_samples_split': 10,...           0.963405   \n",
       "4  {'n_estimators': 300, 'min_samples_split': 8, ...           0.953240   \n",
       "\n",
       "   split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "0           0.965820           0.960102         0.962262        0.002535   \n",
       "1           0.963914           0.963278         0.962346        0.001786   \n",
       "2           0.966836           0.965184         0.965735        0.000779   \n",
       "3           0.966201           0.965947         0.965184        0.001262   \n",
       "4           0.951715           0.952986         0.952647        0.000667   \n",
       "\n",
       "   rank_test_score  \n",
       "0              100  \n",
       "1               97  \n",
       "2               21  \n",
       "3               38  \n",
       "4              119  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rslt_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the randomize grid search cv result\n",
    "rslt_df.to_csv('RF_randgridsearchcv_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted f1 score: 0.05314315749646595\n"
     ]
    }
   ],
   "source": [
    "# use the best parameters , compare performance\n",
    "#classifier1=RandomForestClassifier(n_estimators=120,random_state=12,max_features=10,max_depth=22,bootstrap=False,min_samples_split=5,verbose=0)\n",
    "#classifier1.fit(X,yy)\n",
    "pred_y_test=rf_random.best_estimator_.predict(X_test)\n",
    "# assess the model performance: obtain prescision/recall/f1 scores\n",
    "a=precision_recall_fscore_support(y_test, pred_y_test)\n",
    "f1s=a[2]\n",
    "#prec_rec+=[a[0]]\n",
    "#recal_rec+=[a[1]]\n",
    "# create the weighted f1 score: larger emphasis on high-votes bin and medium\n",
    "# adjusted f1_forall= (f1_low+f1_mid*2+f1_high*3)/6\n",
    "adjf1=(f1s[0]+f1s[1]*2+f1s[2]*3)/6\n",
    "print(\"weighted f1 score:\",adjf1)\n",
    "#wf1_rec+=[adjf1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in the testing data:\n",
      "number of low votes: 9483\n",
      "number of medium votes: 318\n",
      "number of high votes: 63\n",
      "number of features in X: 25\n"
     ]
    }
   ],
   "source": [
    "print(\"in the testing data:\")\n",
    "print(\"number of low votes:\",list(y_test).count(0))\n",
    "print(\"number of medium votes:\",list(y_test).count(1))\n",
    "print(\"number of high votes:\",list(y_test).count(2))\n",
    "print(\"number of features in X:\",len(ndf.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[9434   38   11]\n",
      " [ 207  104    7]\n",
      " [  32   13   18]]\n",
      "other metrics:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98      9483\n",
      "           1       0.67      0.33      0.44       318\n",
      "           2       0.50      0.29      0.36        63\n",
      "\n",
      "    accuracy                           0.97      9864\n",
      "   macro avg       0.72      0.54      0.60      9864\n",
      "weighted avg       0.96      0.97      0.96      9864\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print confusion matrix-- testing\n",
    "print(\"confusion matrix:\",confusion_matrix(y_test, pred_y_test))\n",
    "print(\"other metrics:\",classification_report(y_test, pred_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the different scalars on the features\n",
    "# (1) MinMaxScalar\n",
    "# (2) StandardScalar\n",
    "# (3) RobustScalar\n",
    "from sklearn.preprocessing import StandardScaler,RobustScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(udf_X, y, test_size=0.3, stratify=y, random_state=12)\n",
    "# redo the benchmark model\n",
    "scalar1 = StandardScaler()\n",
    "scalar1.fit(ndf)\n",
    "X1 = scalar1.transform(ndf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spare the X_test with proper versions\n",
    "X_test=X_test[usecolsX]\n",
    "X_test1=scalar1.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted f1 score: 0.4945016196986005\n"
     ]
    }
   ],
   "source": [
    "# use the best parameters , compare performance\n",
    "classifier2=RandomForestClassifier(n_estimators=120,random_state=12,max_features=10, max_depth=22,min_samples_split=5,bootstrap=False,verbose=0)\n",
    "classifier2.fit(X1,yy)\n",
    "pred_y_test=classifier2.predict(X_test1)\n",
    "# assess the model performance: obtain prescision/recall/f1 scores\n",
    "a=precision_recall_fscore_support(y_test, pred_y_test)\n",
    "f1s=a[2]\n",
    "prec_rec+=[a[0]]\n",
    "recal_rec+=[a[1]]\n",
    "# create the weighted f1 score: larger emphasis on high-votes bin and medium\n",
    "# adjusted f1_forall= (f1_low+f1_mid*2+f1_high*3)/6\n",
    "adjf1=(f1s[0]+f1s[1]*2+f1s[2]*3)/6\n",
    "print(\"weighted f1 score:\",adjf1)\n",
    "wf1_rec+=[adjf1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[9440   36    7]\n",
      " [ 208  104    6]\n",
      " [  34   12   17]]\n",
      "other metrics:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      9483\n",
      "           1       0.68      0.33      0.44       318\n",
      "           2       0.57      0.27      0.37        63\n",
      "\n",
      "    accuracy                           0.97      9864\n",
      "   macro avg       0.74      0.53      0.60      9864\n",
      "weighted avg       0.96      0.97      0.96      9864\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print confusion matrix-- testing\n",
    "print(\"confusion matrix:\",confusion_matrix(y_test, pred_y_test))\n",
    "print(\"other metrics:\",classification_report(y_test, pred_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test the Robust Scalar\n",
    "scalar2 = RobustScaler()\n",
    "scalar2.fit(ndf)\n",
    "X2 = scalar2.transform(ndf)\n",
    "X_test2=scalar2.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted f1 score: 0.4927467704889388\n"
     ]
    }
   ],
   "source": [
    "# use the bechmark parameters , compare performance\n",
    "classifier3=RandomForestClassifier(n_estimators=120,random_state=12,max_features=10, max_depth=22,min_samples_split=5,bootstrap=False,verbose=0)\n",
    "classifier3.fit(X2,yy)\n",
    "pred_y_test=classifier3.predict(X_test2)\n",
    "# assess the model performance: obtain prescision/recall/f1 scores\n",
    "a=precision_recall_fscore_support(y_test, pred_y_test)\n",
    "f1s=a[2]\n",
    "prec_rec+=[a[0]]\n",
    "recal_rec+=[a[1]]\n",
    "# create the weighted f1 score: larger emphasis on high-votes bin and medium\n",
    "# adjusted f1_forall= (f1_low+f1_mid*2+f1_high*3)/6\n",
    "adjf1=(f1s[0]+f1s[1]*2+f1s[2]*3)/6\n",
    "print(\"weighted f1 score:\",adjf1)\n",
    "wf1_rec+=[adjf1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix: [[9438   38    7]\n",
      " [ 209  103    6]\n",
      " [  34   12   17]]\n",
      "other metrics:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98      9483\n",
      "           1       0.67      0.32      0.44       318\n",
      "           2       0.57      0.27      0.37        63\n",
      "\n",
      "    accuracy                           0.97      9864\n",
      "   macro avg       0.74      0.53      0.60      9864\n",
      "weighted avg       0.96      0.97      0.96      9864\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print confusion matrix-- testing\n",
    "print(\"confusion matrix:\",confusion_matrix(y_test, pred_y_test))\n",
    "print(\"other metrics:\",classification_report(y_test, pred_y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
